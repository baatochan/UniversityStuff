%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Machine Learning Techniques in Network Optimization Problems}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Bartosz Rodziewicz}
\IEEEauthorblockA{nr indeksu: 226105\\
Politechnika Wrocławska}
\and
\IEEEauthorblockN{Adam Hyjek}
\IEEEauthorblockA{nr indeksu: 234987\\
Politechnika Wrocławska}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Network optimization is an important aspect in todays network studies. Last years have bringed increasing demands on networks' capacity and causes its exhausting. Among many methods used to prevent it some uses artificial intelligence methods, which is also gaining interest in research community. We believe that appliance of AI methods, especially machine learning has big potential in solving network optimization problems. In this paper we created review of currently used machine learning techniques and algorithms in various network optimization problems. We reviewed 23 articles concerning 5 network optimization problems and proposing enhancements for its solutions. We believe that there is still room for improvements and big potential in machine learning appliance in solving network optimization problems.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
In recent years we can observe increasing amount of new devices connected to various networks. This trend is expected to increase further with 5G release and IoT concept gaining popularity. This causes increased network traffic leading to exhausting networks' capacity. To prevent that, there are numerous techniques of network optimization. Some of the most popular methods of network optimization discussed in the article are edge caching, real-time routing, load balancing, traffic shaping and data compression.\par
Edge caching is a single-sided storing of popular, recently viewed content allowing to reduce number of subsequent requests to external network. It allows not only to reduce external traffic, but also decreases response time significantly. Biggest problem in edge caching is predicting future demand for data and choosing what content already existing in cache should be replaced by incoming one. Most popular approaches are two simple algorithms: Least recently used (LRU) and Least frequently used (LFU)\cite{cache1}\cite{cache2}.\par
Real-time routing is a process where data is forwarded to it's destination based on current network condition, allowing route path to stay relevant even despite constant changes in network condition. There is a big variety in algorithms analyzing current network state such as RIP or EIGRP, but performing optimal routing based only on current network situation is a complex task. Routing results can be improved though by taking into account previous experiences to avoid some recurring problems\cite{routing1}\cite{routing2}.\par
Load balancing is a process of distributing network traffic across multiple servers to avoid overloading particular server leading to deterioration of service quality or even complete service degradation for some users. Typical algorithms used in load balancing problems are Round Robin algorithms and solutions based on current state's metrics, but just as in routing problem, making decision may be significantly improved by analyzing past circumstances and staying away from former issues\cite{balancing1}\cite{balancing2}.\par
Traffic shaping is an optimization technique which delays some of the packets to bring the overall traffic into the desired profile. It is used to optimize performance, increase usable bandwidth or improve latency for other applications and users. Since most of the network traffic is encrypted nowadays, the need to quickly and accurately classify Internet traffic for this purpose has been growing steadily\cite{shaping1}. This is a field where machine learning comes into play and provides a way to classify the encrypted network traffic.\par
Data compression is a subject that goes far beyond the network optimization problems. It is a process of encoding information using fewer bits than original representation. There are two types of data compression - lossy and lossless. The lossy compression reduce the amount of bits used to store information by removing less important information. The lossless compression reduces bits by identifying and eliminating statistical redundancy. There exists many different data compression algorithm, both lossy and lossless, and some of them use machine learning to increase their efficiency\cite{compression1}\cite{compression2}.\par
Machine learning is a subset of artificial intelligence. It is a study of algorithms that improve automatically over time. Machine learning algorithms use statistics to find patterns in massive amounts of data and make predictions based on that data. The more data the model has for training, the better the prediction are.\par
In this article we focus on machine learning usage in network optimization problems mentioned above. The article's goal is to point out increasing popularity of artificial intelligence methods applied to solving network optimization related issues and their success rate. The rest of this survey is organized into 5 sections dedicated to methods of network optimization and section containing our conclusion.\par


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Edge Caching}
Edge caching strategy depends on solving four fundamental issues: where, how, when and what to cache\cite{cache3}. First issue, also called Cache placement, can be divided into infrastructure caching and infrastructureless caching. In infrastructureless caching content is stored at user equipment such as mobile devices and home routers. It lowers network load and latency, but drastically reduces amount of users using particular caching solution. On the other hand, infrastructure caching takes place in various parts of public network, for example in Base Transceiver Station (BTS), which causes higher latency when accessing cached data, but increases chances that content is already stored\cite{cache3}.\par
Caching policy determines how data is cached and is split into proactive and reactive approach. Reactive policy is a simpler one and means that uncached user request is immediately saved. Most of the time this approach is applied in many layers of network, meaning that data is progressively requested from next network layer if not found locally. Proactive policy is based on predicting popularity and prefetching data probably needed in the future. This approach can potentially greatly increase network quality and decrease its load, but also can result in large decrease of cache hit ratio, depending on algorithm quality. Therefore it's great opportunity to use machine learning algorithms which are characterised by improving self over time\cite{cache3}.\par
There are many machine learning techniques used in edge caching, such as supervised learning, unsupervised learning, reinforcement learning, neural networks, similarity learning and transfer learning\cite{cache4}.\par
Supervised learning is used most commonly to predict traffic intensity and content demand and to decide which content should be cached. From supervised learning methods most used are classification and regression analysis\cite{cache1}. Examples of usage are: predicting popularity of new videos and maximization of cache hit ratio with usage of learn-to-rank algorithm\cite{cache3}.\par
Most often exploited technique from unsupervised learning for caching solution is clustering, which is used to aggregate user equipments (UEs) into groups based on their previous requests. Thanks to this identification data demands can be predicted by comparing them with other users from the same group\cite{cache1}.\par
Transfer learning based methods for caching solutions are utilized for estimating content's future demand and appealing based on user-content correlations from another related domains\cite{cache1}. Transfer learning can be also used for transferring knowledge from relative tasks to speed up learning process and avoid cold start\cite{cache3}.\par
Reinforcement learning is usually used to solve subproblem of other machine learning based techniques, because of its delayed reward and trial and error approach. Learning process can be modeled as Markov Decision Process and can be used to minimize cache replacement transmission cost or solve cache replacement problem\cite{cache3}.\par
Deep learning can be used for both edge caching and caching related content delivery. It can provide good results comparing to other machine learning approaches, but on the other hand it may require big amount of data to train the model\cite{cache1}.\par
Similarity learning is used to create similarity function based on two given pairs: one of similar objects and second consisting objects with lesser level of similarity. Such created function is then used to judge if incoming content is similar to recently popular ones among user equipments\cite{cache1}. It can also be used to investigate how similar are preferences of users, what then leads to estimating content demand.\par

\section{Real-Time Routing}
While network traffic growths rapidly, most of the networks themselves use old routing frameworks designed for fixed networks, which are characterized by relatively stable state of links, which most of the times leads to easy to calculate and predict routes\cite{routing1}. In wireless networks, in contrast to fixed ones, network link states become much more unstable and it becomes hard to calculate optimal routes quickly.\par
To evaluate possible routes there is a need to define some Quality of Service metrics that have to be satisfied to ensure proper quality of solution. There are two important routing measures that are used during designing routing protocols: packet delivery ratio (PDR) and bandwidth utilization. Packet delivery ratio is defined as a ratio of number of data packets received to data packets send. Reliable routing protocols should ensure PDR value of 100\% or at least nearly 100\%. Good bandwidth utilization is achieved by limiting transmission of not necessary data and and network control overhead\cite{routing3}.\par
The advantage of machine learning methods when applied to real-time routing problem is ability to predict and formulate future routing states, policies and protocols by observing previous ones\cite{routing4}. Although potential of machine learning methods in routing is very high, there are several problems that need to be addressed when creating such solutions. First of all, it is not trivial to translate routing problems into machine learning solvable problems. Secondly, machine learning algorithms need to be trained, what creates computational delay, which is undesirable regarding short flow transmission time. Last, but not least, computational power of machine learning approach may not meet requirements of network routing optimization's needs such as accuracy, robustness and scalability\cite{routing4}.\par
Among machine learning methods that are being applied to real-time routing protocols we can mention supervised learning, unsupervised learning and reinforcement learning\cite{routing3}. While in supervised learning samples carry label information, in unsupervised learning sample data is unlabeled. Supervised learning is used in classification, decision making and regression, while unsupervised learning is utilized in data clustering and data dimensionality reduction. In opposition to two already mentioned approaches, reinforcement learning determines what should be done based on obtained data. Taken actions are then judged by received reward, what allows to train the model. Trained model can then be used for prediction\cite{routing4}.\par
Supervised learning algorithms for real-time routing optimization problems uses network and traffic states as input of the training datasets, while routing solutions delivered by corresponding heuristic algorithm are the output. Such routing architecture consists of machine learning based meta-layer and heuristic algorithm layer, which output is used for training machine learning layer. After training phase, routing decision are taken by machine-learning based meta-layer independently and optimal heuristic-like solutions can be obtained then in real-time.\cite{routing3}.\par
Unsupervised learning is often organized as Self-Organizing Map (SOM) and is used to reduce dimensionality and data clustering. SOM consist of an input layer and map layer. Map layer include neurons with weight vectors, which are being compared with input data sample. Vector with highest similarity and its neighbourhood is then adjusted towards input data. Whole process is repeated until convergence condition is met\cite{routing3}.\par
Reinforcements learning based algorithms in general are used to solve decision making problems. In routing optimization problem for SDN networks controller takes agent role, while network is described as environment, state space is composed of network and traffic states and routing solution is described as action. Reward in such system is based on chosen optimization metric. Such constructed model can work efficiently even in highly chaotic environments\cite{routing3}.\par
Traffic prediction's goal is to predict trend of traffic by analysis past traffic information. It allows to establish proactive routing policies to foresee incoming traffic and avoid traffic congestion, which in result improves network's Quality of Service. Most commonly machine learning solution for traffic prediction are neural networks predicting network traffic load. This prediction is used to calculate optimal resource allocation using non-machine learning algorithms\cite{routing3}.\par

\section{Load Balancing}
Load balancing is a technique with much broader scope than just its use by Internet services. However this paper focuses on that and how Internet services can benefit from its applications. The main ways of approaching it in this scenario are: the simple DNS based and the more complex server-side based.\par
In the DNS-based load balancing technique one domain is assigned to multiple IP addresses (representing multiple servers) and DNS server randomly chooses the server that will be provided to the client. In average usage this method offers similar split of traffic between the servers but there is no guarantee for that. Different factors may degrade the quality of this technique, such as DNS record caching, failure of one server or big differences in load caused by individual requests\cite{balancing-rfc}.\par
The server-side based technique is usually achieved by usage of specialized software created for load balancing which works as a kind of proxy for communication between a client and a chosen server. This method provides a way to have a full control over how load balancing is done and aspects such as scheduling algorithm or persistence. This is also a technique that could benefit from machine learning in some of its layers\cite{balancing2}\cite{balancing3}.\par
The classic approach to server-side load balancing is a software that uses a heuristic algorithm to decide which server to choose based on server stats. This data can consist of its capacity, current load, CPU utilization and many more\cite{balancing1}. The machine learning approaches often suggest changing the heuristic algorithm to the one based on machine learning which can make better prediction based on this data\cite{balancing3}.\par
The advantage of machine learning techniques when applied to load balancers is ability to predict how the utilization of particular server will change in the future by analyzing the past. Some of the algorithms can also analyze how different types of request cause change in the server load. The data used for analysis are machine learning friendly which makes machine learning solutions suitable for this task\cite{balancing3}.
There are few machine learning techniques used among load balancing solutions that are worth mentioning: supervised\cite{balancing3}\cite{balancing5}, unsupervised\cite{balancing5}\cite{balancing4} and reinforced\cite{balancing6} learning. The supervised learning takes advantage of historical labeled data, while unsupervised can discover the hidden pattern behind the selected features.\par
The supervised learning can be used for prediction of amount of traffic coming to the servers and to predict which type of request will use up more load on the server\cite{balancing5}.
The unsupervised and reinforced learning is used for analysis of server stats and making decision which server should get the request. The unsupervised one creates the patterns between server load utilization and ability to take the request while reinforced learning additional benefit from the system of rewards. The system of rewards in reinforced learning technique often is based on algorithm's ability to keep the load evenly split between the servers\cite{balancing5}\cite{balancing6}.\par
Other worth mentioning machine learning approach to the load balancing problem is ML algorithm that doesn't split load by itself but focuses on selecting the best heuristic load balancing algorithm for given circumstances and specific use. This method mostly uses supervised learning and few heuristic algorithms\cite{balancing7}.\par

\section{Traffic Shaping}
Traffic shaping techniques require fast and accurate methods for classification of the traffic flow. Some time ago data traffic could be classified based on just port number or protocol. However since most of the traffic is encrypted nowadays, new traffic classification methods are required\cite{shaping1}\cite{shaping2}.\par
Machine learning based approach is independent from packet payload inspection and provides a way to analyze and classify encrypted data which can't be easily distinguished with typical properties. Because of that majority of modern traffic classification algorithms rely on machine learning. Most popular machine learning techniques used in traffic classification are supervised\cite{shaping1} and unsupervised\cite{shaping3} learning and both of them have its own pros and cons\cite{shaping2}.\par
Supervised machine learning algorithms require a training phase on previously labeled traffic flows to create connection between traffic classes and applications. Because of that, supervised machine learning can be useful for the identification of specific set of application. However, it is worth mentioning that supervised learning algorithm works best when trained on data sets containing all the traffic classes the algorithm is expected to see in practise. Its performance may degrade when the algorithm is trained on data set vastly different than the traffic flows seen when operating. When evaluating supervised learning algorithm for potential use it is worth considering how the classifier will be re-train if needed and how new type of applications will be detected\cite{shaping2}.\par
One of potential benefit of unsupervised machine learning algorithm may be automatic discovery of traffic classes (clusters) in the dataset. However these automatically created classes still have to be labelled and mapped to application (mostly through inspection by human expert). Another benefit may be creation of a new traffic class when a new application starts to play significant role on the market yet before it spotted by human experts. On the other hand big issue when using unsupervised ML algorithms is that clusters often don't map 1:1 to application. It is common that one application may spread over few clusters and dominate them while another application spread over but doesn't dominate any cluster. When evaluating unsupervised learning algorithm for potential use it is worth considering how clusters will be mapped to specific application, how new applications will be detected and labels updated and what is the optimal number of clusters\cite{shaping2}.\par
Another challenges that these algorithms have to overcome are timely and continuous classification, directional neutrality, efficient use of memory and processors, portability and robustness.\par

\section{Data Compression}
Data compression is a topic that is only briefly related to a network optimization and a field that doesn't use many machine learning algorithms (it is more common the other way around\cite{compression3}). However, it was chosen to be included in this paper because it is a field which may start to play a bigger role as development and usage of IoT devices progresses\cite{compression2}.\par
The lossless compression algorithms based on machine learning may better recognize and optimize redundant data. However, the compression/decompression software have to be pre-trained for use with specific type of data. Popular machine learning techniques used in traffic classification are unsupervised learning and deep learning\cite{compression1}\cite{compression4}.\par
Both of these methods focuses on learning to find similar patterns and redundancy in data being compressed. The deep learning while generally getting better results requires a lot more data to train which leads to bigger size of the model used by compressing software to run\cite{compression4}.\par
The lossy compression algorithms focuses on finding patterns that allows to get rid of some data without much drop in quality eg. by compressing data into single equation. The most commonly used machine learning technique in lossy compression algorithms is unsupervised learning\cite{compression2}.

\section{Conclusions}
Network optimization is very important aspect today, but we predict it will become even more important in coming years. Another dynamically developing study is machine learning which is experiencing a renaissance with recent advances in computer science. It seems natural to combine both studies to improve networks' quality and capacity. In this paper we explore methods of network optimization and try to find examples of machine learning approaches used for it. \par
We have reviewed 23 articles from 5 network optimization areas: Edge caching, real-time routing, load balancing, traffic shaping and data compression. In edge caching we described caching issues and approaches to solve them and also uses of various machine learning techniques. In case of real-time routing we discussed QoS metrics and reviewed machine learning approaches based on these metrics. Regarding load balancing we focused on Internet services load balancing including server-side based techniques and traffic prediction. In traffic shaping chapter we explained application areas of machine learning and its usage and benefits. In data compression section we described compression algorithms as well as unsupervised learning and deep learning usage in this area.\par
The trends we discovered and described are very promising, but we believe there is still room for improvement. We expect even higher growth of interest in this area, especially with the release of 5G-based networks allowing popular concept of Internet of Things. 
% conference papers do not normally have an appendix


% use section* for acknowledgment
% \section*{Acknowledgment}



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{cache1}
Z. Chang, L. Lei, Z. Zhou, S. Mao and T. Ristaniemi, "Learn to Cache: Machine Learning for Network Edge Caching in the Big Data Era," in IEEE Wireless Communications, vol. 25, no. 3, pp. 28-35, JUNE 2018, doi: 10.1109/MWC.2018.1700317.
% https://ieeexplore.ieee.org/abstract/document/8403948
\bibitem{cache2}
H. Zhu, Y. Cao, W. Wang, T. Jiang and S. Jin, "Deep Reinforcement Learning for Mobile Edge Caching: Review, New Features, and Open Issues," in IEEE Network, vol. 32, no. 6, pp. 50-57, November/December 2018, doi: 10.1109/MNET.2018.1800109.
% https://ieeexplore.ieee.org/abstract/document/8553654
\bibitem{routing1}
F. Tang et al., "On Removing Routing Protocol from Future Wireless Networks: A Real-time Deep Learning Approach for Intelligent Traffic Control," in IEEE Wireless Communications, vol. 25, no. 1, pp. 154-160, February 2018, doi: 10.1109/MWC.2017.1700244.
% https://ieeexplore.ieee.org/abstract/document/8088549
\bibitem{routing2}
S. Troia et al., "Machine-Learning-Assisted Routing in SDN-Based Optical Networks," 2018 European Conference on Optical Communication (ECOC), Rome, 2018, pp. 1-3, doi: 10.1109/ECOC.2018.8535437.
% https://ieeexplore.ieee.org/abstract/document/8535437
\bibitem{balancing1}
R. A. Haidri, C. P. Katti and P. C. Saxena, "A load balancing strategy for Cloud Computing environment," 2014 International Conference on Signal Propagation and Computer Technology (ICSPCT 2014), Ajmer, 2014, pp. 636-641, doi: 10.1109/ICSPCT.2014.6884914.
% https://ieeexplore.ieee.org/document/6884914
\bibitem{balancing2}
Anna Victoria C R Oikawa, Vinicius Freitas, Márcio Castro, Laércio Lima Pilla. Adaptive Load
Balancing based on Machine Learning for Iterative Parallel Applications. 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP), Mar 2020, Västerås,
Sweden. ff10.1109/PDP50117.2020.00021ff. ffhal-02570549f
% https://hal.archives-ouvertes.fr/hal-02570549/document
\bibitem{shaping1}
T. S. Tabatabaei, F. Karray and M. Kamel, "Early internet traffic recognition based on machine learning methods," 2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), Montreal, QC, 2012, pp. 1-5, doi: 10.1109/CCECE.2012.6335034.
% https://ieeexplore.ieee.org/abstract/document/6335034
\bibitem{compression1}
F. H. Kingma, P. Abbeel and J. Ho, "Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables" 2019.
% https://arxiv.org/abs/1905.06845v4
% https://bair.berkeley.edu/blog/2019/09/19/bit-swap/
\bibitem{compression2}
J. Park, H. Park and Y. Choi, "Data compression and prediction using machine learning for industrial IoT," 2018 International Conference on Information Networking (ICOIN), Chiang Mai, 2018, pp. 818-820, doi: 10.1109/ICOIN.2018.8343232.
% https://ieeexplore.ieee.org/document/8343232
\bibitem{cache3}
Shuja, Junaid, K. Bilal, Eisa A. Alanazi, W. Alasmary and A. Alashaikh. “Applying Machine Learning Techniques for Caching in Edge Networks: A Comprehensive Survey.” ArXiv abs/2006.16864 (2020): n. pag.
% https://arxiv.org/pdf/2006.16864.pdf
\bibitem{cache4}
Y. Sun, M. Peng, Y. Zhou, Y. Huang and S. Mao, "Application of Machine Learning in Wireless Networks: Key Techniques and Open Issues," in IEEE Communications Surveys \& Tutorials, vol. 21, no. 4, pp. 3072-3108, Fourthquarter 2019, doi: 10.1109/COMST.2019.2924243.
% https://ieeexplore.ieee.org/abstract/document/8743390
\bibitem{routing3}
O. Ashour, M. St-Hilaire, T. Kunz and M. Wang, "A Survey of Applying Reinforcement Learning Techniques to Multicast Routing," 2019 IEEE 10th Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference (UEMCON), New York City, NY, USA, 2019, pp. 1145-1151, doi: 10.1109/UEMCON47517.2019.8993014.
% https://ieeexplore.ieee.org/document/8993014
\bibitem{routing4}
K. Yu, L. Tan, X. Wu and Z. Gai, "Machine Learning Driven Network Routing," 2019 6th International Conference on Systems and Informatics (ICSAI), Shanghai, China, 2019, pp. 705-712, doi: 10.1109/ICSAI48974.2019.9010507.
% https://ieeexplore.ieee.org/document/9010507
\bibitem{balancing-rfc}
Brisco, T., "DNS Support for Load Balancing", RFC 1794, doi: 10.17487/RFC1794, April 1995.
% https://tools.ietf.org/html/rfc1794
\bibitem{balancing3}
S. Parida and B. Panchal, "An Efficient Dynamic Load Balancing Algorithm Using Machine Learning Technique in Cloud Environment", International journal of scientific research in science, engineering and technology, vol. 4, p. 1184-1186, 2018.
% http://ijsrset.com/paper/4504.pdf
\bibitem{balancing5}
C. Gomez, A. Shami and X. Wang, "Machine Learning Aided Scheme for Load Balancing in Dense IoT Networks", Sensors, vol. 18, p. 3779, 2018.
% https://www.researchgate.net/publication/328750804_Machine_Learning_Aided_Scheme_for_Load_Balancing_in_Dense_IoT_Networks
\bibitem{balancing4}
A. Revar, M. Andhariya, D. Sutariya and M. Bhavsar. “Load Balancing in Grid Environment using Machine Learning - Innovative Approach.” International Journal of Computer Applications, vol. 8, p. 24-28, 2010.
% https://www.researchgate.net/publication/49586510_Load_Balancing_in_Grid_Environment_using_Machine_Learning_-_Innovative_Approach
\bibitem{balancing6}
A. Schaerf, Y. Shoham and M. Tennenholtz, "Adaptive Load Balancing: A Study in Multi-Agent Learning", Journal of Artificial Intelligence Research, vol. 2, doi: 10.1613/jair.121, 1995.
% https://www.jair.org/index.php/jair/article/view/10133/24002
\bibitem{balancing7}
C. R. Anna Victoria Oikawa, V. Freitas, M. Castro and L. L. Pilla, "Adaptive Load Balancing based on Machine Learning for Iterative Parallel Applications," 2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP), Västerås, Sweden, 2020, pp. 94-101, doi: 10.1109/PDP50117.2020.00021.
% https://ieeexplore.ieee.org/abstract/document/9092148
\bibitem{shaping2}
T. T. T. Nguyen and G. Armitage, "A survey of techniques for internet traffic classification using machine learning," in IEEE Communications Surveys \& Tutorials, vol. 10, no. 4, pp. 56-76, Fourth Quarter 2008, doi: 10.1109/SURV.2008.080406.
% https://ieeexplore.ieee.org/abstract/document/6335034
\bibitem{shaping3}
S. Zander, T. Nguyen and G. Armitage, "Automated traffic classification and application identification using machine learning," The IEEE Conference on Local Computer Networks 30th Anniversary (LCN'05)l, Sydney, NSW, 2005, pp. 250-257, doi: 10.1109/LCN.2005.35.
% https://ieeexplore.ieee.org/abstract/document/1550864
\bibitem{compression3}
D. Sculley and C. E. Brodley, "Compression and machine learning: a new perspective on feature space vectors," Data Compression Conference (DCC'06), Snowbird, UT, 2006, pp. 332-341, doi: 10.1109/DCC.2006.13.
% https://ieeexplore.ieee.org/document/1607268
\bibitem{compression4}
Mohit Goyal, Kedar Tatwawadi, Shubham Chandak and Idoia Ochoa, "DeepZip: Lossless Data Compression using Recurrent Neural Networks.", 2018.
% https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2761006.pdf
% https://arxiv.org/abs/1811.08162

\end{thebibliography}




% that's all folks
\end{document}
